{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9294510f-3e5b-4b80-b112-1d3e1778cc0f",
   "metadata": {},
   "source": [
    "## Question 1: What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288db6a5-43be-4c20-89ab-3fc716e14b6e",
   "metadata": {},
   "source": [
    "The mathematical formula for a linear Support Vector Machine (SVM) involves finding a hyperplane that best separates the data points of different classes in a dataset. The goal is to maximize the margin, which is the distance between the hyperplane and the closest data points from either class.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given a set of training data \\(\\{(x_i, y_i)\\}_{i=1}^n\\), where \\(x_i \\in \\mathbb{R}^n\\) is an n-dimensional feature vector and \\(y_i \\in \\{-1, 1\\}\\) is the class label, the decision boundary can be defined as:\n",
    "\n",
    "\\[ w^T x + b = 0 \\]\n",
    "\n",
    "Where:\n",
    "- \\(w\\) is the weight vector perpendicular to the hyperplane.\n",
    "- \\(x\\) is the feature vector.\n",
    "- \\(b\\) is the bias term.\n",
    "\n",
    "The decision function, which determines the classification, can be written as:\n",
    "\n",
    "\\[ f(x) = \\text{sign}(w^T x + b) \\]\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "The objective of the linear SVM is to find the weight vector \\(w\\) and the bias \\(b\\) that maximize the margin between the hyperplane and the closest data points from each class. This problem can be formulated as a constrained optimization problem:\n",
    "\n",
    "**Primal Form:**\n",
    "\n",
    "Minimize the objective function:\n",
    "\n",
    "\\[ \\frac{1}{2} \\|w\\|^2 \\]\n",
    "\n",
    "Subject to the constraints:\n",
    "\n",
    "\\[ y_i (w^T x_i + b) \\geq 1 \\quad \\forall i = 1, \\ldots, n \\]\n",
    "\n",
    "The term \\(\\frac{1}{2} \\|w\\|^2\\) is minimized to ensure that the margin is maximized. The constraints ensure that all data points are correctly classified with a margin of at least 1.\n",
    "\n",
    "### Lagrange Dual Formulation\n",
    "\n",
    "To solve the optimization problem, the Lagrange dual problem can be used, which introduces Lagrange multipliers \\(\\alpha_i\\) for each constraint:\n",
    "\n",
    "\\[ L(w, b, \\alpha) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w^T x_i + b) - 1] \\]\n",
    "\n",
    "The solution to the SVM problem involves finding the set of \\(\\alpha_i\\) that maximize the dual objective function under the constraints:\n",
    "\n",
    "\\[ \\sum_{i=1}^{n} \\alpha_i y_i = 0 \\quad \\text{and} \\quad \\alpha_i \\geq 0 \\quad \\forall i \\]\n",
    "\n",
    "The weight vector \\(w\\) can then be obtained as:\n",
    "\n",
    "\\[ w = \\sum_{i=1}^{n} \\alpha_i y_i x_i \\]\n",
    "\n",
    "The bias \\(b\\) can be determined by using the support vectors, which are the data points for which the constraints are exactly met (i.e., the points lying on the margin).\n",
    "\n",
    "### Decision Rule\n",
    "\n",
    "Once \\(w\\) and \\(b\\) are determined, the decision rule for classifying a new data point \\(x\\) is:\n",
    "\n",
    "\\[ f(x) = \\text{sign}(w^T x + b) \\]\n",
    "\n",
    "This decision rule assigns the data point \\(x\\) to the positive class (+1) if \\(f(x) > 0\\) and to the negative class (-1) if \\(f(x) < 0\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74cbd1-aa75-4f8e-bb76-8e44f4dcd296",
   "metadata": {},
   "source": [
    "## Question 2: What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb29fbd-618e-4a64-b180-e56a4a39873d",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) aims to find the hyperplane that best separates the data points of different classes while maximizing the margin between the hyperplane and the nearest data points (support vectors) of each class. The objective function consists of two parts: the regularization term and the loss function.\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "The primary objective function of a linear SVM is:\n",
    "\n",
    "\\[ \\min_{w, b} \\quad \\frac{1}{2} \\|w\\|^2 \\]\n",
    "\n",
    "Subject to the constraints:\n",
    "\n",
    "\\[ y_i (w^T x_i + b) \\geq 1 \\quad \\forall i = 1, \\ldots, n \\]\n",
    "\n",
    "Where:\n",
    "- \\(w\\) is the weight vector perpendicular to the hyperplane.\n",
    "- \\(x_i\\) is the feature vector of the \\(i\\)-th training sample.\n",
    "- \\(y_i\\) is the class label of the \\(i\\)-th training sample, which can be either +1 or -1.\n",
    "- \\(b\\) is the bias term.\n",
    "- \\(\\|w\\|\\) denotes the Euclidean norm of the weight vector \\(w\\).\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Regularization Term (\\(\\frac{1}{2} \\|w\\|^2\\))**: This term aims to minimize the norm of the weight vector \\(w\\), effectively maximizing the margin between the classes. By minimizing \\(\\|w\\|\\), the model finds the hyperplane that maximizes the distance from the nearest data points, thus enhancing the generalization ability of the model.\n",
    "\n",
    "- **Constraints (\\(y_i (w^T x_i + b) \\geq 1\\))**: These constraints ensure that all data points are correctly classified with a margin of at least 1. For each data point, the product \\(y_i (w^T x_i + b)\\) must be greater than or equal to 1. This condition ensures that the data points of the positive class lie on one side of the hyperplane (where \\(w^T x + b \\geq 1\\)), and the data points of the negative class lie on the other side (where \\(w^T x + b \\leq -1\\)).\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The objective function of a linear SVM focuses on maximizing the margin between the classes by minimizing the norm of the weight vector \\(w\\), subject to the constraints that all training samples are correctly classified with a margin of at least 1. This formulation leads to a convex optimization problem, which ensures that the solution is globally optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be520480-fa79-4353-9d9b-c57d7fa1b1d5",
   "metadata": {},
   "source": [
    "## Question 3 : What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca43c0b-f81d-4770-ae0c-ed2a20c52f2b",
   "metadata": {},
   "source": [
    "The **kernel trick** in Support Vector Machines (SVMs) is a technique used to enable the algorithm to classify data that is not linearly separable in the original feature space. The kernel trick allows SVMs to work in a high-dimensional (possibly infinite-dimensional) space without explicitly computing the coordinates of the data in that space. Instead, it relies on the computation of inner products in the transformed feature space.\n",
    "\n",
    "### Concept of the Kernel Trick\n",
    "\n",
    "In SVMs, the goal is to find a hyperplane that separates the data points of different classes. However, in many real-world scenarios, the data cannot be separated by a linear hyperplane in the original feature space. The kernel trick addresses this issue by implicitly mapping the input features into a higher-dimensional space, where a linear separation may become possible.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given a transformation \\(\\phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) that maps the original features \\(x \\in \\mathbb{R}^n\\) to a higher-dimensional space \\(\\phi(x) \\in \\mathbb{R}^m\\), the decision function in this space is:\n",
    "\n",
    "\\[ f(x) = \\text{sign}(w^T \\phi(x) + b) \\]\n",
    "\n",
    "However, explicitly computing \\(\\phi(x)\\) can be computationally expensive, especially when the dimensionality \\(m\\) is very high or infinite. The kernel trick allows us to avoid this explicit computation by using a **kernel function** \\(K(x, x')\\), which directly computes the inner product in the transformed feature space:\n",
    "\n",
    "\\[ K(x, x') = \\langle \\phi(x), \\phi(x') \\rangle \\]\n",
    "\n",
    "### Common Kernel Functions\n",
    "\n",
    "1. **Linear Kernel**:\n",
    "   \\[ K(x, x') = x^T x' \\]\n",
    "   This corresponds to the original feature space without any transformation.\n",
    "\n",
    "2. **Polynomial Kernel**:\n",
    "   \\[ K(x, x') = (x^T x' + c)^d \\]\n",
    "   This kernel represents a polynomial combination of the original features.\n",
    "\n",
    "3. **Radial Basis Function (RBF) or Gaussian Kernel**:\n",
    "   \\[ K(x, x') = \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\sigma^2}\\right) \\]\n",
    "   This kernel maps the data into an infinite-dimensional space and is capable of handling highly non-linear relationships.\n",
    "\n",
    "4. **Sigmoid Kernel**:\n",
    "   \\[ K(x, x') = \\tanh(\\alpha x^T x' + c) \\]\n",
    "   This kernel is related to neural networks.\n",
    "\n",
    "### Advantages of the Kernel Trick\n",
    "\n",
    "1. **Non-linearity**: It enables SVMs to handle non-linear relationships by transforming the data into a higher-dimensional space where a linear separation is possible.\n",
    "2. **Efficiency**: It avoids the explicit computation of the transformation \\(\\phi(x)\\), which can be computationally expensive, especially for high-dimensional mappings.\n",
    "3. **Flexibility**: Different kernel functions can be used to capture various types of relationships in the data.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The kernel trick is a powerful method in SVMs that allows them to perform well on non-linear classification tasks by using kernel functions to implicitly map the data into a higher-dimensional space. This technique enables SVMs to find complex decision boundaries without the computational burden of working in high-dimensional spaces explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e93d6a-4162-43d0-81a5-6a58e418e47b",
   "metadata": {},
   "source": [
    "## Question 4: What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c250dca-b9f8-4e72-8c9a-8ad4a6927a2b",
   "metadata": {},
   "source": [
    "In Support Vector Machines (SVMs), **support vectors** are the data points that lie closest to the decision boundary (hyperplane). These points are critical in defining the position and orientation of the hyperplane that separates the classes. The role of support vectors is essential because they are the only data points that influence the placement of the hyperplane; the other data points are not involved in the determination of the optimal hyperplane.\n",
    "\n",
    "### Role of Support Vectors\n",
    "\n",
    "1. **Defining the Margin**: Support vectors are the data points that are exactly on the edge of the margin. The margin is the distance between the hyperplane and the nearest data points of each class. The width of the margin is maximized during the training process, and the support vectors are the points that lie on the boundary of this margin. The distance between the hyperplane and these support vectors is exactly 1 in the normalized SVM formulation.\n",
    "\n",
    "2. **Optimal Hyperplane**: The optimal hyperplane is determined by the support vectors. If any of these support vectors are moved, the position of the hyperplane would change. Thus, the support vectors are critical for defining the decision boundary. \n",
    "\n",
    "3. **Model Complexity**: The number of support vectors can indicate the complexity of the model. A small number of support vectors often means a simpler model, while a large number of support vectors might indicate a more complex model.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a simple binary classification problem with two features where we want to classify points into two classes: Class 1 and Class 2. Let's say we have a dataset with the following features and class labels:\n",
    "\n",
    "- **Class 1**: Points at (1, 2), (2, 3)\n",
    "- **Class 2**: Points at (4, 5), (5, 6)\n",
    "\n",
    "When we apply an SVM to this dataset, the SVM algorithm will find a hyperplane that separates the two classes with the maximum margin. Suppose the hyperplane is determined to be:\n",
    "\n",
    "\\[ w^T x + b = 0 \\]\n",
    "\n",
    "Where \\( w \\) is the weight vector and \\( b \\) is the bias term.\n",
    "\n",
    "- The **support vectors** might be the points (2, 3) from Class 1 and (4, 5) from Class 2. These are the points closest to the hyperplane.\n",
    "- The hyperplane is positioned such that the margin between the hyperplane and these support vectors is maximized.\n",
    "- The support vectors are crucial for computing the optimal \\( w \\) and \\( b \\). Moving these points would affect the position of the hyperplane, which would, in turn, affect the classification of the other points.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "Imagine a 2D plot where the points from Class 1 are blue and the points from Class 2 are red. The support vectors are the points that are just touching the margin lines (parallel lines to the hyperplane). The hyperplane will be equidistant from these margin lines, ensuring that the margin is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273951ac-5fbf-4aff-a9c9-0666bbdfddf7",
   "metadata": {},
   "source": [
    "## Question 5: Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb321849-3c9d-42b6-858b-11b4d9ad02fd",
   "metadata": {},
   "source": [
    "To illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in Support Vector Machines (SVM), let's use a simple 2D example with graphical representations. \n",
    "\n",
    "### 1. Hyperplane\n",
    "\n",
    "**Definition**: The hyperplane in SVM is the decision boundary that separates data points of different classes. In a 2D space, it is a straight line.\n",
    "\n",
    "**Example**: Suppose we have two classes of data points:\n",
    "\n",
    "- **Class 1**: Points at (1, 2) and (2, 3)\n",
    "- **Class 2**: Points at (4, 5) and (5, 6)\n",
    "\n",
    "**Graph**: \n",
    "A hyperplane can be represented as a line that separates the two classes. For instance:\n",
    "\n",
    "\\[ w^T x + b = 0 \\]\n",
    "\n",
    "Where \\(w\\) is the weight vector and \\(b\\) is the bias. This line will ideally be positioned between the two classes, ensuring that it divides them as effectively as possible.\n",
    "\n",
    "![Hyperplane](https://www.svm-tutorial.com/images/linear-separation-2D.png)\n",
    "\n",
    "### 2. Marginal Planes\n",
    "\n",
    "**Definition**: The marginal planes are parallel to the hyperplane and are positioned at a distance from it equal to the margin. The margin is the distance between these two marginal planes and is maximized during training.\n",
    "\n",
    "**Example**: For the same dataset, the marginal planes can be represented as:\n",
    "\n",
    "\\[ w^T x + b = +1 \\] (for Class 1)\n",
    "\\[ w^T x + b = -1 \\] (for Class 2)\n",
    "\n",
    "**Graph**:\n",
    "\n",
    "The margin is the space between these two planes:\n",
    "\n",
    "![Marginal Planes](https://www.svm-tutorial.com/images/margin.png)\n",
    "\n",
    "### 3. Hard Margin\n",
    "\n",
    "**Definition**: In hard margin SVM, the data is assumed to be perfectly linearly separable. The goal is to find a hyperplane that maximizes the margin between the two classes while ensuring that all data points are correctly classified with no errors.\n",
    "\n",
    "**Example**: If the dataset is perfectly separable, you can draw a line (hyperplane) between the two classes such that all points of Class 1 lie on one side of the hyperplane and all points of Class 2 lie on the other side.\n",
    "\n",
    "**Graph**:\n",
    "\n",
    "![Hard Margin](https://www.svm-tutorial.com/images/hard-margin.png)\n",
    "\n",
    "### 4. Soft Margin\n",
    "\n",
    "**Definition**: In soft margin SVM, the model allows some misclassifications to handle cases where the data is not perfectly linearly separable. The goal is to find a hyperplane that maximizes the margin while allowing for some flexibility in classification to handle outliers or overlaps.\n",
    "\n",
    "**Example**: If some points from Class 1 are on the wrong side of the margin or if some points overlap, a soft margin SVM introduces slack variables to allow these misclassifications and adjusts the hyperplane accordingly.\n",
    "\n",
    "**Graph**:\n",
    "\n",
    "In the case of a soft margin, some data points might fall within the margin or on the wrong side of the hyperplane, but the model still aims to maximize the margin as much as possible.\n",
    "\n",
    "![Soft Margin](https://www.svm-tutorial.com/images/soft-margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e2ce1-798e-4d1b-b49c-07ac897a0b3e",
   "metadata": {},
   "source": [
    "## Question 6: SVM Implementation through Iris dataset.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352d097-3717-41e3-9c4a-738c230b17b5",
   "metadata": {},
   "source": [
    "Here’s a detailed guide on how to implement a linear SVM classifier using the Iris dataset, including a bonus task to compare a custom implementation with scikit-learn's implementation.\n",
    "\n",
    "### 1. Load the Iris Dataset and Split into Training and Testing Sets\n",
    "\n",
    "First, let's load the Iris dataset and split it into training and testing sets.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "### 2. Train a Linear SVM Classifier and Predict Labels\n",
    "\n",
    "#### Using scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm = SVC(kernel='linear', C=1.0)  # C is the regularization parameter\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of scikit-learn SVM: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "### 3. Plot Decision Boundaries\n",
    "\n",
    "To visualize decision boundaries, we'll use two features of the Iris dataset. For simplicity, let’s use the first two features.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Plotting decision boundaries\n",
    "def plot_decision_boundaries(X, y, model, title):\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', s=20, cmap=ListedColormap(['#FF0000', '#00FF00', '#0000FF']))\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Select the first two features for visualization\n",
    "X_train_2d = X_train[:, :2]\n",
    "X_test_2d = X_test[:, :2]\n",
    "\n",
    "# Train the SVM model on the 2D features\n",
    "svm_2d = SVC(kernel='linear', C=1.0)\n",
    "svm_2d.fit(X_train_2d, y_train)\n",
    "\n",
    "# Plot decision boundaries\n",
    "plot_decision_boundaries(X_train_2d, y_train, svm_2d, 'Decision Boundaries of Linear SVM')\n",
    "```\n",
    "\n",
    "### 4. Custom Linear SVM Implementation\n",
    "\n",
    "Here’s a simple implementation of a linear SVM classifier from scratch using gradient descent. For this example, we'll use a basic form of SVM without advanced features like kernels or optimization techniques.\n",
    "\n",
    "```python\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, C=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.C = C\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        y = np.where(y <= 1, -1, 1)  # Convert labels to -1 and 1\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for idx, xi in enumerate(X):\n",
    "                if y[idx] * (np.dot(xi, self.weights) + self.bias) < 1:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights - np.dot(xi, y[idx]))\n",
    "                    self.bias -= self.learning_rate * y[idx]\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * 2 * self.C * self.weights\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "# Train the custom SVM model\n",
    "svm_custom = LinearSVM(learning_rate=0.001, epochs=1000, C=1.0)\n",
    "svm_custom.fit(X_train_2d, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred_custom = svm_custom.predict(X_test_2d)\n",
    "\n",
    "# Compute the accuracy of the custom model\n",
    "accuracy_custom = np.mean(y_pred_custom == y_test)\n",
    "print(f\"Accuracy of custom SVM: {accuracy_custom:.2f}\")\n",
    "```\n",
    "\n",
    "### 5. Compare Performance with Different Regularization Parameters\n",
    "\n",
    "To compare the performance with different values of the regularization parameter \\(C\\), you can repeat the training and evaluation steps for different values of \\(C\\):\n",
    "\n",
    "```python\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "for C in C_values:\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with C={C}: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Loaded the Iris dataset** and split it into training and testing sets.\n",
    "2. **Trained a linear SVM classifier** using scikit-learn and computed accuracy.\n",
    "3. **Plotted decision boundaries** for a 2D subset of features.\n",
    "4. **Implemented a simple linear SVM classifier from scratch** and compared its performance with scikit-learn's implementation.\n",
    "5. **Explored different values of the regularization parameter \\(C\\)** and observed its effect on model performance.\n",
    "\n",
    "This approach provides a comprehensive understanding of linear SVMs, from practical implementation to theoretical insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9dd692-fec0-4880-976f-2fd21b3271e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
