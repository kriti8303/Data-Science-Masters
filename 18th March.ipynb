{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b26d830-9fa9-46b7-bf8c-c9d33ddd4e56",
   "metadata": {},
   "source": [
    "## Question 1: What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce165ca6-49e7-4ba3-8686-48e10a686611",
   "metadata": {},
   "source": [
    "The Filter method in feature selection is a technique that evaluates the relevance of features independently of any machine learning algorithm. It uses statistical techniques to assess the relationship between each feature and the target variable, selecting features based on their individual merits.\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "* Statistical Measurement: Features are evaluated based on statistical measures such as correlation, Chi-square, Mutual Information, ANOVA F-value, etc. These measures assess how well each feature correlates with the target variable.\n",
    "\n",
    "* Ranking Features: Features are ranked based on the chosen statistical metric. For example, features with higher correlation values or lower p-values might be considered more relevant.\n",
    "\n",
    "* Selection: Based on the ranking or threshold criteria, a subset of features is selected for further analysis or model training. Features that do not meet the criteria are discarded.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. Model Independence: It is not dependent on the choice of the machine learning algorithm.\n",
    "2. Computational Efficiency: Generally faster as it only involves statistical calculations rather than training models.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "1. No Interaction Consideration: It does not account for interactions between features, which might be important in some cases.\n",
    "2. Potential Overlook: It might overlook features that are individually weak but valuable in combination with other features.\n",
    "\n",
    "### Example:\n",
    "\n",
    "If you are using correlation as a metric, you would calculate the correlation coefficient between each feature and the target variable. Features with higher absolute correlation values would be selected for inclusion in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d717174-afa4-492f-9c1f-aef477b61871",
   "metadata": {},
   "source": [
    "## Question 2: How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db76b7-6b52-449d-995a-4c8c0b588a69",
   "metadata": {},
   "source": [
    "The Wrapper method and the Filter method are both feature selection techniques, but they differ significantly in their approach and process.\n",
    "\n",
    "### Wrapper Method:\n",
    "\n",
    "* Model-Specific: The Wrapper method evaluates subsets of features by training and testing a machine learning model on these subsets. It directly measures the performance of the model using different feature subsets to determine their effectiveness.\n",
    "\n",
    "* Subset Evaluation: It involves creating various combinations of features (subsets) and evaluating each subset's performance based on a chosen metric, such as accuracy, precision, recall, etc.\n",
    "\n",
    "* Computational Cost: Typically more computationally expensive because it requires training and evaluating the model multiple times for different feature subsets.\n",
    "\n",
    "* Feature Interactions: It considers interactions between features, as it evaluates feature subsets in the context of model performance.\n",
    "\n",
    "### Example: \n",
    "If you are using a Wrapper method, you might use techniques such as Recursive Feature Elimination (RFE), where the model is trained repeatedly while removing the least important features until the optimal subset is found.\n",
    "\n",
    "### Filter Method:\n",
    "\n",
    "* Model-Independent: The Filter method evaluates individual features independently of any machine learning algorithm. It uses statistical measures to assess the relevance of each feature to the target variable.\n",
    "\n",
    "* Feature Ranking: It ranks features based on their statistical significance or correlation with the target variable and selects a subset of features based on this ranking.\n",
    "\n",
    "* Computational Cost: Generally less computationally expensive since it involves only statistical calculations and does not require model training.\n",
    "\n",
    "* No Interaction Consideration: It does not consider interactions between features, focusing only on individual feature relevance.\n",
    "\n",
    "### Example: \n",
    "Using the Filter method, you might select features based on their correlation with the target variable or by using statistical tests like Chi-square tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e227d7-5c16-4943-96f1-9375f1180fa5",
   "metadata": {},
   "source": [
    "## Question 3: What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ed7ec-0e88-43dc-be54-103801d9135d",
   "metadata": {},
   "source": [
    "Embedded feature selection methods integrate feature selection directly into the model training process. They evaluate feature importance as part of the model's learning algorithm, combining the benefits of both filter and wrapper methods. Here are some common techniques used in embedded feature selection:\n",
    "\n",
    "1. Lasso Regression (L1 Regularization):\n",
    "\n",
    "* Description: Lasso regression adds a penalty proportional to the absolute value of the coefficients to the loss function. This regularization can shrink some feature coefficients to zero, effectively performing feature selection by excluding those features.\n",
    "\n",
    "* Usage: Commonly used with linear regression models to perform both feature selection and regularization.\n",
    "\n",
    "2. Ridge Regression (L2 Regularization):\n",
    "\n",
    "* Description: Ridge regression adds a penalty proportional to the square of the coefficients. While it does not set coefficients to zero (thus not performing feature selection), it can still help in feature ranking by reducing the influence of less important features.\n",
    "\n",
    "* Usage: Often used when multicollinearity is an issue, and it can be combined with other methods to improve feature selection.\n",
    "\n",
    "3. Elastic Net:\n",
    "\n",
    "* Description: Elastic Net combines both L1 and L2 regularization penalties. It can perform feature selection like Lasso while also handling collinearity like Ridge regression.\n",
    "\n",
    "* Usage: Useful when there are many correlated features.\n",
    "\n",
    "4. Decision Trees and Tree-Based Models:\n",
    "\n",
    "* Description: Tree-based algorithms like Decision Trees, Random Forests, and Gradient Boosted Trees inherently perform feature selection. They evaluate the importance of each feature based on how much they reduce impurity (e.g., Gini impurity, entropy) in the model.\n",
    "\n",
    "* Usage: Feature importance can be extracted from models like Random Forests or Gradient Boosted Trees and used for feature selection.\n",
    "\n",
    "5. Feature Importance from Models:\n",
    "\n",
    "* Description: Models like XGBoost, LightGBM, and CatBoost provide feature importance scores as part of their output. These scores can be used to select the most relevant features based on their importance to the model's predictions.\n",
    "\n",
    "* Usage: Feature importance scores are used to rank and select features, with higher importance features being chosen for the model.\n",
    "\n",
    "6. Regularization Techniques:\n",
    "\n",
    "* Description: Various regularization techniques incorporated in algorithms (such as Lasso in linear models) help in performing feature selection by adding penalties that shrink some feature coefficients to zero.\n",
    "\n",
    "* Usage: Regularization is used to prevent overfitting and to select important features by excluding less relevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde72144-29e0-4b3f-9145-50ec0c7302ab",
   "metadata": {},
   "source": [
    "## Question 4: What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e0c7d5-51c3-47b5-8455-405b582236bc",
   "metadata": {},
   "source": [
    "The Filter method for feature selection has several advantages, such as being computationally efficient and model-agnostic. However, it also has some drawbacks:\n",
    "\n",
    "1. Ignores Feature Interactions:\n",
    "\n",
    "* Description: The Filter method evaluates each feature independently based on statistical metrics (e.g., correlation with the target variable) without considering interactions between features.\n",
    "* Impact: Important interactions between features may be overlooked, which can lead to suboptimal feature selection and potentially reduce model performance.\n",
    "\n",
    "2. Not Model-Specific:\n",
    "\n",
    "* Description: Filter methods do not take into account how features affect the performance of a specific machine learning model.\n",
    "* Impact: Features selected by Filter methods might not necessarily contribute to the performance of the chosen model, as the selection criteria are not tied to the model's learning process.\n",
    "\n",
    "3. Over-Simplification:\n",
    "\n",
    "* Description: Filter methods use simple statistical tests or metrics (e.g., correlation coefficients, chi-square tests) to assess feature importance.\n",
    "* Impact: This can lead to an oversimplification of feature importance, potentially missing out on complex relationships or dependencies that a more sophisticated method might capture.\n",
    "\n",
    "4. Limited to Statistical Measures:\n",
    "\n",
    "* Description: The Filter method primarily relies on statistical measures to evaluate features, such as correlation, mutual information, or statistical tests.\n",
    "* Impact: Features that have a complex or non-linear relationship with the target variable might not be selected if the statistical measure does not capture these relationships effectively.\n",
    "\n",
    "5. No Interaction with Model Training:\n",
    "\n",
    "* Description: Since Filter methods are applied before model training, they do not benefit from feedback based on the model’s performance.\n",
    "* Impact: This lack of interaction with model training can result in features being selected based on criteria that do not necessarily lead to better model performance.\n",
    "\n",
    "6. Risk of Redundant Features:\n",
    "\n",
    "* Description: Filter methods may select features that are redundant or highly correlated with each other if these features individually score well in statistical tests.\n",
    "* Impact: Redundant features can introduce multicollinearity and may not provide additional value to the model, potentially reducing interpretability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93ad1e-c87f-4679-b210-db015d42a584",
   "metadata": {},
   "source": [
    "## Question 5: In which situations would you prefer using the Filter method over the Wrapper method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230a400-95ca-474f-b739-b80b191edbf6",
   "metadata": {},
   "source": [
    "The Filter method is often preferred over the Wrapper method in various situations due to its characteristics and advantages. Here are some scenarios where the Filter method would be more suitable:\n",
    "\n",
    "1. High-Dimensional Datasets:\n",
    "\n",
    "* Situation: When dealing with datasets with a very large number of features, such as text data or genomics data.\n",
    "* Reason: The Filter method is computationally efficient and can quickly eliminate irrelevant features based on statistical measures, reducing the dimensionality before applying more computationally intensive methods.\n",
    "\n",
    "2. Limited Computational Resources:\n",
    "\n",
    "* Situation: When computational resources are constrained, and there is a need for a quick and efficient feature selection process.\n",
    "* Reason: Filter methods are less computationally expensive compared to Wrapper methods, which involve training and evaluating multiple models.\n",
    "\n",
    "3. Initial Feature Selection:\n",
    "\n",
    "* Situation: During the initial stages of feature selection when the goal is to narrow down the feature set before applying more complex methods.\n",
    "* Reason: Filter methods can provide a preliminary reduction in feature space, making it easier to apply Wrapper or Embedded methods on a smaller set of features.\n",
    "\n",
    "4. Model Independence:\n",
    "\n",
    "* Situation: When feature selection needs to be performed independently of any specific machine learning model.\n",
    "* Reason: The Filter method evaluates features based on statistical metrics rather than model-specific performance, making it suitable for scenarios where the model is not yet determined or needs to be agnostic.\n",
    "\n",
    "5. Simple Relationships:\n",
    "\n",
    "* Situation: When the relationships between features and the target variable are expected to be relatively simple or linear.\n",
    "* Reason: Filter methods work well with simple statistical measures, which are effective if the relationships between features and the target variable are straightforward.\n",
    "\n",
    "6. Quick Assessment:\n",
    "\n",
    "* Situation: When a rapid assessment of feature importance is needed without the need for extensive model training.\n",
    "* Reason: Filter methods provide quick insights into feature relevance based on statistical tests or metrics, allowing for immediate insights into which features might be valuable.\n",
    "\n",
    "7. Exploratory Data Analysis:\n",
    "\n",
    "* Situation: During exploratory data analysis to identify potentially useful features before further analysis.\n",
    "* Reason: The Filter method can help quickly identify and eliminate features that do not have a strong correlation with the target variable, streamlining the process for more detailed analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd2c32-a2eb-416a-8499-0d7422ec6161",
   "metadata": {},
   "source": [
    "## Question 6: In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c899f57-17c8-42aa-87ee-b7225e34acff",
   "metadata": {},
   "source": [
    "To choose the most pertinent attributes for a predictive model for customer churn using the Filter Method, you would follow these steps:\n",
    "\n",
    "1. Understand the Dataset:\n",
    "\n",
    "* Examine the Data: Review the dataset to understand the different features available and their types (e.g., categorical, numerical).\n",
    "* Identify Target Variable: Ensure you have a clear understanding of the target variable, which in this case is customer churn (usually a binary variable indicating whether a customer has churned or not).\n",
    "\n",
    "2. Preprocess the Data:\n",
    "\n",
    "* Handle Missing Values: Address any missing values in the dataset through imputation or removal.\n",
    "* Normalize/Standardize Data: If features are on different scales, normalize or standardize numerical features to ensure fair comparison.\n",
    "\n",
    "3. Select Statistical Measures:\n",
    "\n",
    "Choose appropriate statistical measures based on the type of features and target variable:\n",
    "\n",
    "* For Numerical Features: Use correlation coefficients (e.g., Pearson correlation) to measure the linear relationship between numerical features and the target variable.\n",
    "* For Categorical Features: Use statistical tests such as Chi-square tests to evaluate the relationship between categorical features and the target variable.\n",
    "\n",
    "4. Calculate Feature Scores:\n",
    "\n",
    "* Correlation Coefficients: Compute the correlation coefficient between each numerical feature and the target variable. Features with high absolute correlation values (close to 1 or -1) are more relevant.\n",
    "* Chi-Square Test: For categorical features, perform the Chi-square test to assess the independence of features from the target variable. Features with low p-values (typically < 0.05) are considered significant.\n",
    "\n",
    "5. Rank Features:\n",
    "\n",
    "* Rank by Correlation: Rank numerical features based on their correlation coefficients with the target variable.\n",
    "* Rank by Chi-Square Score: Rank categorical features based on their Chi-square test results or p-values.\n",
    "\n",
    "6. Select Relevant Features:\n",
    "\n",
    "* Set a Threshold: Decide on a threshold for feature importance based on the scores or p-values. For example, select features with correlation values above a certain threshold or p-values below a significance level.\n",
    "* Eliminate Irrelevant Features: Remove features that do not meet the threshold criteria or show weak relationships with the target variable.\n",
    "\n",
    "7. Evaluate Feature Selection:\n",
    "\n",
    "* Review Selected Features: Check if the selected features make sense based on domain knowledge and their relevance to customer churn.\n",
    "* Perform Exploratory Analysis: Conduct exploratory data analysis to ensure that selected features provide meaningful insights and are not highly redundant.\n",
    "\n",
    "8. Document and Implement:\n",
    "\n",
    "* Document Feature Selection: Keep a record of the features selected and the criteria used for selection.\n",
    "* Implement in Model: Use the selected features to build and train your predictive model for customer churn.\n",
    "\n",
    "### Example Code Snippet:\n",
    "Here’s a simplified example of how you might use the Filter Method in Python with Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4a9e2-c1ed-49f8-8000-8a8d6058bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('customer_churn.csv')\n",
    "\n",
    "# Assuming 'churn' is the target variable and others are features\n",
    "\n",
    "# For numerical features\n",
    "correlations = df.corr()['churn'].abs().sort_values(ascending=False)\n",
    "print(\"Feature Correlations:\\n\", correlations)\n",
    "\n",
    "# For categorical features\n",
    "def chi2_test(feature):\n",
    "    contingency_table = pd.crosstab(df[feature], df['churn'])\n",
    "    chi2_stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    return p_value\n",
    "\n",
    "categorical_features = ['feature1', 'feature2']  # Replace with actual categorical features\n",
    "p_values = {feature: chi2_test(feature) for feature in categorical_features}\n",
    "p_values = sorted(p_values.items(), key=lambda x: x[1])\n",
    "print(\"Chi-Square P-values:\\n\", p_values)\n",
    "\n",
    "# Select features based on thresholds\n",
    "selected_features = correlations[correlations > 0.1].index.tolist()  # Example threshold for correlation\n",
    "selected_features.extend([feature for feature, p in p_values if p < 0.05])  # Example threshold for p-value\n",
    "\n",
    "print(\"Selected Features:\\n\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887a323c-e2e7-42cc-98e4-9483977f9229",
   "metadata": {},
   "source": [
    "## Question 7 : You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3ef90-20f2-42e7-ac24-186a51b2ecba",
   "metadata": {},
   "source": [
    "To use the Embedded method for feature selection in a project predicting the outcome of a soccer match, you would follow these steps:\n",
    "\n",
    "1. Understand the Dataset:\n",
    "\n",
    "* Examine Features: Review the dataset, which includes features like player statistics, team rankings, and possibly other contextual data.\n",
    "* Define Target Variable: Identify the target variable, which could be the match outcome (win, loss, or draw).\n",
    "\n",
    "2. Preprocess the Data:\n",
    "\n",
    "* Handle Missing Values: Address missing data through imputation or removal.\n",
    "* Normalize/Standardize Data: Ensure that features are on a comparable scale, especially if different features have different units or scales.\n",
    "* Encode Categorical Variables: Convert categorical features into numerical format if needed, using techniques like one-hot encoding.\n",
    "\n",
    "3. Choose an Embedded Method:\n",
    "\n",
    "* Embedded methods perform feature selection as part of the model training process. Here are some common techniques:\n",
    "\n",
    "### Lasso Regression (L1 Regularization):\n",
    "\n",
    "* Model Training: Train a Lasso regression model, which includes an L1 penalty term that encourages sparsity in feature coefficients.\n",
    "* Feature Selection: Features with non-zero coefficients are considered relevant. Features with zero coefficients are deemed less important and can be discarded.\n",
    "\n",
    "### Decision Trees and Tree-based Methods (e.g., Random Forest, Gradient Boosting):\n",
    "\n",
    "* Model Training: Train a decision tree-based model. These models inherently provide feature importance scores based on how frequently and effectively each feature splits the data.\n",
    "* Feature Selection: Extract feature importance scores and select features with higher importance values.\n",
    "\n",
    "### Regularized Linear Models:\n",
    "\n",
    "* Model Training: Use models with regularization techniques, such as Ridge (L2 regularization) or Elastic Net (a combination of L1 and L2 regularization).\n",
    "* Feature Selection: Although Ridge regression does not perform feature selection directly, Elastic Net can help in selecting important features by combining L1 and L2 penalties.\n",
    "\n",
    "4. Implement the Embedded Method:\n",
    "\n",
    "* Here's how you can implement Lasso regression and a Random Forest model using Python and scikit-learn:\n",
    "\n",
    "## Using Lasso Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37d16b-da09-4e28-95ae-a78f699e9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('soccer_data.csv')\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop('match_outcome', axis=1)\n",
    "y = df['match_outcome']\n",
    "\n",
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Lasso regression model\n",
    "lasso = Lasso(alpha=0.01)  # alpha is the regularization parameter\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importance = lasso.coef_\n",
    "features = X.columns\n",
    "selected_features = [features[i] for i in range(len(importance)) if importance[i] != 0]\n",
    "\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4927af3-b5ea-409d-8d72-4d0ac911bdee",
   "metadata": {},
   "source": [
    "## Using Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25906f-b8e9-464c-a7dc-d721a6271ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('soccer_data.csv')\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop('match_outcome', axis=1)\n",
    "y = df['match_outcome']\n",
    "\n",
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importance = rf.feature_importances_\n",
    "features = X.columns\n",
    "selected_features = [features[i] for i in range(len(importance)) if importance[i] > 0.01]  # Example threshold\n",
    "\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a91eb-bfd5-4a89-afd5-978eb2ac25ab",
   "metadata": {},
   "source": [
    "5. Evaluate Feature Selection:\n",
    "\n",
    "* Review Results: Analyze the selected features to ensure they make sense in the context of soccer match predictions.\n",
    "* Validate Model: Assess model performance with the selected features using metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "6. Document and Implement:\n",
    "\n",
    "* Document: Keep a record of the features selected and the rationale behind their selection.\n",
    "* Implement: Use the selected features in your final predictive model and validate its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c52ee-0ae7-44d3-a2fa-45a9455cfa4b",
   "metadata": {},
   "source": [
    "## Question 8: You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700a39d-e959-4fe7-a1f2-5ff5864f92ec",
   "metadata": {},
   "source": [
    "To use the Wrapper method for feature selection in a project to predict house prices, follow these steps:\n",
    "\n",
    "1. Understand the Dataset:\n",
    "\n",
    "* Review Features: Your dataset includes features like size, location, and age of the house.\n",
    "* Define Target Variable: The target variable is the price of the house.\n",
    "\n",
    "2. Preprocess the Data:\n",
    "\n",
    "* Handle Missing Values: Address any missing values in your features or target variable.\n",
    "* Normalize/Standardize Data: Ensure that features are on a comparable scale if necessary.\n",
    "* Encode Categorical Variables: Convert categorical features (like location) into numerical format using techniques such as one-hot encoding.\n",
    "\n",
    "3. Choose a Wrapper Method:\n",
    "\n",
    "The Wrapper method evaluates feature subsets by training and testing a model. Common techniques include:\n",
    "\n",
    "* Forward Selection: Start with no features, and iteratively add features that improve model performance.\n",
    "* Backward Elimination: Start with all features, and iteratively remove features that do not contribute significantly to model performance.\n",
    "* Recursive Feature Elimination (RFE): Use a model to recursively remove the least important features until the desired number of features is reached.\n",
    "\n",
    "4. Implement the Wrapper Method:\n",
    "\n",
    "Here’s how you can use Recursive Feature Elimination (RFE) with a regression model using Python and scikit-learn:\n",
    "\n",
    "### Using RFE with a Linear Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f792f23-cdee-40dd-a6e6-ad57cdd070b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load or create dataset\n",
    "# Replace with your dataset loading code\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='PRICE')\n",
    "\n",
    "# Preprocess data if needed\n",
    "# For example: Normalize/Standardize, Encode Categorical Variables\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Initialize RFE with the model and the desired number of features\n",
    "rfe = RFE(model, n_features_to_select=5)  # Specify the number of features you want to select\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "\n",
    "print(\"Selected Features:\", selected_features)\n",
    "\n",
    "# Evaluate the model with selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "model.fit(X_train_selected, y_train)\n",
    "score = model.score(X_test_selected, y_test)\n",
    "print(\"Model R^2 score with selected features:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f03cc3-f4f6-4877-aa19-7a9fa34bb7db",
   "metadata": {},
   "source": [
    "5. Evaluate Feature Selection:\n",
    "\n",
    "* Assess Performance: Evaluate how the model performs with the selected features. Metrics such as R^2 score, Mean Absolute Error (MAE), or Root Mean Squared Error (RMSE) can be used.\n",
    "* Compare with Baseline: Compare the performance of the model using selected features with the baseline performance (e.g., using all features or a subset chosen randomly).\n",
    "\n",
    "6. Document and Implement:\n",
    "\n",
    "* Document: Record the features selected and their impact on model performance.\n",
    "* Implement: Use the selected features in your final predictive model and ensure consistent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6461b9-1eef-4703-a3f2-826582a30b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
