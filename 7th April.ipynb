{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c327d6-b3df-44ac-9088-52ee3a4a21a7",
   "metadata": {},
   "source": [
    "## Question 1: What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7c4fa-cf9f-4268-ac14-6b344c7dfc45",
   "metadata": {},
   "source": [
    "In machine learning algorithms, particularly Support Vector Machines (SVMs), polynomial functions and kernel functions are closely related concepts. Here's a detailed explanation of their relationship:\n",
    "\n",
    "### Polynomial Functions\n",
    "\n",
    "A polynomial function is a mathematical expression involving a sum of powers in one or more variables multiplied by coefficients. In the context of machine learning, polynomial functions are often used to map input features into higher-dimensional spaces to capture non-linear relationships. For example, a polynomial function of degree \\(d\\) for a single variable \\(x\\) can be expressed as:\n",
    "\n",
    "\\[ f(x) = a_0 + a_1 x + a_2 x^2 + \\cdots + a_d x^d \\]\n",
    "\n",
    "### Kernel Functions\n",
    "\n",
    "Kernel functions are a general concept used in machine learning to implicitly map input features into a higher-dimensional space without explicitly computing the coordinates in that space. This is useful for algorithms like SVMs that rely on finding hyperplanes for classification or regression. The kernel function computes the dot product between the mapped features in the higher-dimensional space, making it computationally efficient.\n",
    "\n",
    "### Polynomial Kernel\n",
    "\n",
    "The polynomial kernel is a specific type of kernel function that corresponds to polynomial functions. The polynomial kernel function of degree \\(d\\) for two input vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is defined as:\n",
    "\n",
    "\\[ K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x} \\cdot \\mathbf{y} + c)^d \\]\n",
    "\n",
    "where:\n",
    "- \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are input vectors.\n",
    "- \\(c\\) is a constant that shifts the kernel function.\n",
    "- \\(d\\) is the degree of the polynomial.\n",
    "\n",
    "### Relationship\n",
    "\n",
    "- **Polynomial Functions as Feature Mapping**: Polynomial functions can be used to map input features into a higher-dimensional space explicitly. For example, using polynomial features of degree \\(d\\) allows you to create features like \\(x^2\\), \\(xy\\), and so on.\n",
    "\n",
    "- **Polynomial Kernel as Implicit Mapping**: The polynomial kernel function achieves the same result as polynomial feature mapping but in an implicit and computationally efficient manner. Instead of explicitly transforming features into higher dimensions, the kernel function computes the dot product directly in the higher-dimensional space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7a27c-a90c-4717-93c7-84072b3bfa21",
   "metadata": {},
   "source": [
    "## Question 2: How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982b0dd-dad0-4894-a26d-8e0a22e7fe11",
   "metadata": {},
   "source": [
    "To implement an SVM with a polynomial kernel in Python using scikit-learn, you can follow these steps:\n",
    "\n",
    "### 1. **Import Required Libraries**\n",
    "\n",
    "First, import the necessary libraries:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "```\n",
    "\n",
    "### 2. **Load the Dataset**\n",
    "\n",
    "For demonstration, we'll use the Iris dataset. Load the dataset and split it into training and testing sets:\n",
    "\n",
    "```python\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "### 3. **Train an SVM with a Polynomial Kernel**\n",
    "\n",
    "Create an SVM model with a polynomial kernel and train it:\n",
    "\n",
    "```python\n",
    "# Create an SVM model with a polynomial kernel\n",
    "# Degree of the polynomial kernel can be set with the 'degree' parameter\n",
    "poly_svm = SVC(kernel='poly', degree=3, C=1.0, coef0=1)\n",
    "\n",
    "# Train the model\n",
    "poly_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = poly_svm.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of SVM with polynomial kernel: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "### 4. **Plot Decision Boundaries (Optional)**\n",
    "\n",
    "To visualize decision boundaries, we can use a subset of features (for 2D visualization). Let's use the first two features of the Iris dataset:\n",
    "\n",
    "```python\n",
    "# Select the first two features for visualization\n",
    "X_train_2d = X_train[:, :2]\n",
    "X_test_2d = X_test[:, :2]\n",
    "\n",
    "# Train the SVM model with a polynomial kernel on the 2D features\n",
    "poly_svm_2d = SVC(kernel='poly', degree=3, C=1.0, coef0=1)\n",
    "poly_svm_2d.fit(X_train_2d, y_train)\n",
    "\n",
    "# Define a function to plot decision boundaries\n",
    "def plot_decision_boundaries(X, y, model, title):\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', s=20, cmap=ListedColormap(['#FF0000', '#00FF00', '#0000FF']))\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundaries for the 2D features\n",
    "plot_decision_boundaries(X_train_2d, y_train, poly_svm_2d, 'Decision Boundaries of Polynomial SVM')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526b125-f5d5-4d70-8030-c27b45eef798",
   "metadata": {},
   "source": [
    "## Question 3: How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68150dba-e01a-4a15-956c-7d0e79d008ac",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the parameter \\(\\epsilon\\) defines the margin of tolerance where no penalty is given for errors within this margin. Increasing the value of \\(\\epsilon\\) affects the number of support vectors as follows:\n",
    "\n",
    "### Impact of Increasing \\(\\epsilon\\) on Support Vectors\n",
    "\n",
    "1. **Definition of \\(\\epsilon\\)**: In SVR, \\(\\epsilon\\) represents the margin of tolerance where predictions are allowed to deviate from the actual values without incurring a penalty. In other words, if the absolute error between the predicted value and the actual value is less than \\(\\epsilon\\), it is considered acceptable and does not contribute to the loss function.\n",
    "\n",
    "2. **Effect on Support Vectors**:\n",
    "   - **Larger \\(\\epsilon\\)**: When \\(\\epsilon\\) is increased, the margin of tolerance is expanded. This means that more data points fall within the \\(\\epsilon\\)-insensitive tube around the regression function, and therefore, fewer data points will be classified as support vectors. Support vectors are the data points that lie outside this margin and influence the position of the regression function.\n",
    "   - **Smaller \\(\\epsilon\\)**: When \\(\\epsilon\\) is decreased, the margin of tolerance is reduced. This means that fewer data points are within the \\(\\epsilon\\)-insensitive tube, leading to more data points being classified as support vectors because they contribute to the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c6e0a-dd8b-447a-9fb3-73618ebe78a0",
   "metadata": {},
   "source": [
    "## Question 4: How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0e121-778d-4f3e-899a-f87cf7c15963",
   "metadata": {},
   "source": [
    "The performance of Support Vector Regression (SVR) is influenced by several key parameters: the kernel function, the \\( C \\) parameter, the \\( \\epsilon \\) parameter, and the \\( \\gamma \\) parameter (for some kernels). Hereâ€™s how each parameter affects SVR and examples of when you might want to adjust their values:\n",
    "\n",
    "### 1. **Kernel Function**\n",
    "\n",
    "**Function**: The kernel function defines the type of hyperplane used to fit the data in SVR. It transforms the input data into a higher-dimensional space where a linear regression model can be applied.\n",
    "\n",
    "- **Linear Kernel**: Suitable for linearly separable data. It's simple and computationally efficient.\n",
    "- **Polynomial Kernel**: Can capture polynomial relationships between features. Useful when data has polynomial relationships.\n",
    "- **Radial Basis Function (RBF) Kernel**: Good for capturing non-linear relationships. It works well with complex data where relationships are not easily captured with a linear or polynomial function.\n",
    "- **Sigmoid Kernel**: Based on the sigmoid function. Less commonly used but can be suitable for specific non-linearities.\n",
    "\n",
    "**When to Adjust**:\n",
    "- **Increase Complexity**: If your data is non-linear, consider using polynomial or RBF kernels. For more complex data, a polynomial kernel with a higher degree or an RBF kernel with appropriate gamma may be needed.\n",
    "- **Simplify**: If the data is linearly separable, a linear kernel may be sufficient.\n",
    "\n",
    "### 2. **C Parameter**\n",
    "\n",
    "**Function**: The \\( C \\) parameter controls the trade-off between achieving a low error on the training data and minimizing the model complexity (i.e., how much the model should deviate from the training data).\n",
    "\n",
    "- **High \\( C \\)**: Places a high penalty on errors within the training data. The model aims to fit the training data as accurately as possible, which can lead to overfitting.\n",
    "- **Low \\( C \\)**: Places a lower penalty on errors, resulting in a smoother decision boundary. This can lead to underfitting if \\( C \\) is too low.\n",
    "\n",
    "**When to Adjust**:\n",
    "- **Increase \\( C \\)**: If your model is underfitting and you want to fit the training data more closely.\n",
    "- **Decrease \\( C \\)**: If your model is overfitting and you want to make it more generalizable.\n",
    "\n",
    "### 3. **\\(\\epsilon\\) Parameter**\n",
    "\n",
    "**Function**: The \\( \\epsilon \\) parameter defines the margin of tolerance where no penalty is given for errors. It specifies a threshold for how far the predicted values can be from the actual values without affecting the loss.\n",
    "\n",
    "- **Large \\(\\epsilon\\)**: Increases the margin of tolerance, allowing more points to fall within the acceptable error range. This results in fewer support vectors and a simpler model.\n",
    "- **Small \\(\\epsilon\\)**: Decreases the margin of tolerance, making the model more sensitive to errors. This results in more support vectors and a potentially more complex model.\n",
    "\n",
    "**When to Adjust**:\n",
    "- **Increase \\(\\epsilon\\)**: If you want a more tolerant model that fits the data less strictly.\n",
    "- **Decrease \\(\\epsilon\\)**: If you need a model that fits the data more precisely.\n",
    "\n",
    "### 4. **Gamma Parameter (for RBF and Polynomial Kernels)**\n",
    "\n",
    "**Function**: The \\( \\gamma \\) parameter controls the influence of a single training example. It determines how far the influence of a single training example reaches.\n",
    "\n",
    "- **High \\( \\gamma \\)**: Makes the model more sensitive to individual data points, leading to a more complex model that can fit the training data very closely.\n",
    "- **Low \\( \\gamma \\)**: Makes the model more general by considering a broader range of points for each training example, leading to a smoother decision boundary.\n",
    "\n",
    "**When to Adjust**:\n",
    "- **Increase \\( \\gamma \\)**: If the model is underfitting and you want to capture more complex patterns in the data.\n",
    "- **Decrease \\( \\gamma \\)**: If the model is overfitting and you want to make it more general.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b384c8e-fca5-4fc6-986e-38b0a9532053",
   "metadata": {},
   "source": [
    "## Question 5: Assignment:\n",
    "1. Import the necessary libraries and load the dataseg\n",
    "2. Split the dataset into training and testing setZ\n",
    "3. Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "4. Create an instance of the SVC classifier and train it on the training datW\n",
    "5. hse the trained classifier to predict the labels of the testing datW\n",
    "6. Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,precision, recall, F1-scoreK\n",
    "7. Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performanc_\n",
    "8. Train the tuned classifier on the entire dataseg\n",
    "9. Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db7835-63ff-4abd-9943-2f6952bcd6e5",
   "metadata": {},
   "source": [
    "Hereâ€™s a detailed guide on how to perform the assignment using Python and the scikit-learn library. This example demonstrates using a Support Vector Classifier (SVC) with a dataset.\n",
    "\n",
    "### Step-by-Step Assignment Solution\n",
    "\n",
    "#### 1. Import the Necessary Libraries and Load the Dataset\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib  # For saving the model\n",
    "\n",
    "# Load the dataset (replace 'path_to_dataset' with the actual path to your dataset)\n",
    "df = pd.read_csv('path_to_dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "#### 2. Split the Dataset into Training and Testing Sets\n",
    "\n",
    "```python\n",
    "# Assuming 'Outcome' is the target variable\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "#### 3. Preprocess the Data\n",
    "\n",
    "```python\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "#### 4. Create an Instance of the SVC Classifier and Train It\n",
    "\n",
    "```python\n",
    "# Initialize the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "#### 5. Use the Trained Classifier to Predict the Labels of the Testing Data\n",
    "\n",
    "```python\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "```\n",
    "\n",
    "#### 6. Evaluate the Performance of the Classifier\n",
    "\n",
    "```python\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "```\n",
    "\n",
    "#### 7. Tune the Hyperparameters Using GridSearchCV\n",
    "\n",
    "```python\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best Score: {grid_search.best_score_:.2f}')\n",
    "\n",
    "# Use the best model to predict\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "precision_best = precision_score(y_test, y_pred_best)\n",
    "recall_best = recall_score(y_test, y_pred_best)\n",
    "f1_best = f1_score(y_test, y_pred_best)\n",
    "\n",
    "print(f'Tuned Model Accuracy: {accuracy_best:.2f}')\n",
    "print(f'Tuned Model Precision: {precision_best:.2f}')\n",
    "print(f'Tuned Model Recall: {recall_best:.2f}')\n",
    "print(f'Tuned Model F1 Score: {f1_best:.2f}')\n",
    "```\n",
    "\n",
    "#### 8. Train the Tuned Classifier on the Entire Dataset\n",
    "\n",
    "```python\n",
    "# Scale the entire dataset\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = df['Outcome']  # Reload target variable if needed\n",
    "\n",
    "# Train the tuned model on the entire dataset\n",
    "best_model.fit(X_scaled, y)\n",
    "```\n",
    "\n",
    "#### 9. Save the Trained Classifier to a File\n",
    "\n",
    "```python\n",
    "# Save the model to a file\n",
    "joblib.dump(best_model, 'svc_model.pkl')\n",
    "print('Model saved to svc_model.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c5dde-66c8-484a-8466-9b9acc4a67a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
