{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f74fda-fcad-42f4-8bb5-cc4b4c22a647",
   "metadata": {},
   "source": [
    "## Question 1: What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb14f44-6bdb-40d5-9e93-0abd440c5e9f",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental principle in probability theory that describes how to update the probability of a hypothesis based on new evidence. It provides a way to calculate the probability of a hypothesis \\( H \\) given observed evidence \\( E \\), using prior knowledge of \\( H \\) and \\( E \\). \n",
    "\n",
    "### Formula\n",
    "The formula for Bayes' theorem is:\n",
    "\n",
    "\\[\n",
    "P(H \\mid E) = \\frac{P(E \\mid H) \\cdot P(H)}{P(E)}\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( P(H \\mid E) \\) is the **posterior probability**: the probability of the hypothesis \\( H \\) given the evidence \\( E \\).\n",
    "- \\( P(E \\mid H) \\) is the **likelihood**: the probability of observing the evidence \\( E \\) given that the hypothesis \\( H \\) is true.\n",
    "- \\( P(H) \\) is the **prior probability**: the initial probability of the hypothesis \\( H \\) before observing the evidence.\n",
    "- \\( P(E) \\) is the **marginal likelihood** or **evidence**: the total probability of observing the evidence \\( E \\) under all possible hypotheses.\n",
    "\n",
    "### Intuition\n",
    "Bayes' theorem allows us to update our belief about a hypothesis based on new evidence. For instance, if you initially believe that a patient has a certain disease based on prior knowledge (prior probability), Bayes' theorem helps you refine that belief in light of new test results (evidence).\n",
    "\n",
    "### Example\n",
    "Imagine you're trying to diagnose a disease. You know the following:\n",
    "- The prior probability of having the disease (\\(P(Disease)\\)) is 1%.\n",
    "- The likelihood of testing positive given the disease (\\(P(Pos \\mid Disease)\\)) is 90%.\n",
    "- The overall probability of testing positive (\\(P(Pos)\\)) is 10%.\n",
    "\n",
    "Bayes' theorem helps calculate the probability of actually having the disease given a positive test result:\n",
    "\n",
    "\\[\n",
    "P(Disease \\mid Pos) = \\frac{P(Pos \\mid Disease) \\cdot P(Disease)}{P(Pos)}\n",
    "\\]\n",
    "\n",
    "Substitute the values:\n",
    "\n",
    "\\[\n",
    "P(Disease \\mid Pos) = \\frac{0.90 \\cdot 0.01}{0.10} = 0.09\n",
    "\\]\n",
    "\n",
    "So, the probability of having the disease given a positive test result is 9%.\n",
    "\n",
    "### Applications\n",
    "Bayes' theorem is widely used in various fields, including:\n",
    "- **Medical Diagnosis**: Updating the probability of a disease based on test results.\n",
    "- **Spam Filtering**: Calculating the probability that an email is spam based on its content.\n",
    "- **Machine Learning**: Implementing algorithms like Naive Bayes for classification tasks.\n",
    "\n",
    "Bayes' theorem is a powerful tool for updating probabilities and making informed decisions based on new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fcb7f-2162-4dd0-b8b5-073f5f1c9c14",
   "metadata": {},
   "source": [
    "## Question 2: What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df942a7e-18f3-4991-8094-2df8221e62fe",
   "metadata": {},
   "source": [
    "The formula for Bayes' theorem is:\n",
    "\n",
    "\\[\n",
    "P(H \\mid E) = \\frac{P(E \\mid H) \\cdot P(H)}{P(E)}\n",
    "\\]\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( P(H \\mid E) \\) is the **posterior probability**: the probability of the hypothesis \\( H \\) given the evidence \\( E \\).\n",
    "- \\( P(E \\mid H) \\) is the **likelihood**: the probability of observing the evidence \\( E \\) given that the hypothesis \\( H \\) is true.\n",
    "- \\( P(H) \\) is the **prior probability**: the initial probability of the hypothesis \\( H \\) before observing the evidence.\n",
    "- \\( P(E) \\) is the **marginal likelihood** or **evidence**: the total probability of observing the evidence \\( E \\) under all possible hypotheses.\n",
    "\n",
    "### Example of Formula Application\n",
    "If you want to calculate the probability of having a disease (\\(H\\)) given a positive test result (\\(E\\)):\n",
    "\n",
    "- **Prior Probability** (\\(P(H)\\)): The initial probability of having the disease.\n",
    "- **Likelihood** (\\(P(E \\mid H)\\)): The probability of testing positive if you have the disease.\n",
    "- **Marginal Likelihood** (\\(P(E)\\)): The overall probability of testing positive, considering both those with and without the disease.\n",
    "\n",
    "Using Bayes' theorem, you can update your belief about the probability of having the disease based on the test result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0af658-9075-46bf-a377-4472d6475e0f",
   "metadata": {},
   "source": [
    "## Question 3: How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baac00a-3ba2-4b39-b653-3da432aa73b4",
   "metadata": {},
   "source": [
    "Bayes' theorem is used in practice across various fields and applications to update probabilities based on new evidence. Here are some common ways it is applied:\n",
    "\n",
    "### 1. **Medical Diagnosis**\n",
    "- **Application**: To determine the likelihood of a disease given test results.\n",
    "- **Example**: If a patient tests positive for a disease, Bayes' theorem helps calculate the probability that the patient actually has the disease, considering the accuracy of the test and the disease's prevalence in the population.\n",
    "\n",
    "### 2. **Spam Filtering**\n",
    "- **Application**: To classify emails as spam or not spam based on their content.\n",
    "- **Example**: By evaluating the probability of certain words appearing in spam and non-spam emails, Bayes' theorem helps filter out spam messages.\n",
    "\n",
    "### 3. **Predictive Modeling**\n",
    "- **Application**: In machine learning algorithms, such as Naive Bayes classifiers.\n",
    "- **Example**: Predicting the class of an item based on its features by calculating the posterior probability of each class given the observed features.\n",
    "\n",
    "### 4. **Financial Forecasting**\n",
    "- **Application**: To update predictions of stock prices or economic indicators based on new data.\n",
    "- **Example**: Adjusting forecasts for stock prices based on recent economic reports and historical data.\n",
    "\n",
    "### 5. **Risk Assessment**\n",
    "- **Application**: In insurance and finance to assess risk based on historical data.\n",
    "- **Example**: Calculating the probability of a financial loss or insurance claim based on past occurrences and current conditions.\n",
    "\n",
    "### 6. **Decision Making**\n",
    "- **Application**: To make informed decisions based on uncertain information.\n",
    "- **Example**: Updating the probability of an event happening (like winning a game) as new information (such as player performance) becomes available.\n",
    "\n",
    "### 7. **Natural Language Processing**\n",
    "- **Application**: In text analysis and language understanding tasks.\n",
    "- **Example**: Estimating the probability of a word or phrase being used in a particular context based on the occurrence of words in a given dataset.\n",
    "\n",
    "### Practical Steps for Applying Bayes' Theorem:\n",
    "1. **Define Hypotheses and Evidence**: Clearly define the hypothesis \\(H\\) and evidence \\(E\\) you are interested in.\n",
    "2. **Calculate Prior Probability**: Determine the initial probability of the hypothesis.\n",
    "3. **Determine Likelihood**: Estimate the probability of observing the evidence given the hypothesis.\n",
    "4. **Compute Marginal Likelihood**: Calculate the overall probability of observing the evidence.\n",
    "5. **Apply Bayes' Theorem**: Use the formula to update the probability of the hypothesis based on the new evidence.\n",
    "\n",
    "### Example\n",
    "In medical diagnosis:\n",
    "- **Prior Probability**: The probability of having a disease before testing (e.g., 1% prevalence).\n",
    "- **Likelihood**: The probability of a positive test result if the disease is present (e.g., 90% sensitivity).\n",
    "- **Evidence**: The total probability of a positive test result (e.g., considering both those with and without the disease).\n",
    "\n",
    "Bayes' theorem helps refine the probability of having the disease given the test result, providing a more accurate assessment than the initial prior probability alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb1588-7f32-4d03-b0ba-47c16dfcc4ba",
   "metadata": {},
   "source": [
    "## Question 4: What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90802f-3e5d-4918-b73f-ebbc6fde00db",
   "metadata": {},
   "source": [
    "Bayes' theorem and conditional probability are closely related concepts in probability theory. Here's how they are connected:\n",
    "\n",
    "### Conditional Probability\n",
    "- **Definition**: Conditional probability measures the probability of an event occurring given that another event has already occurred. It is denoted as \\( P(A \\mid B) \\), which is read as \"the probability of \\( A \\) given \\( B \\).\"\n",
    "- **Formula**: \n",
    "  \\[\n",
    "  P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "  \\]\n",
    "  where \\( P(A \\cap B) \\) is the joint probability of both events \\( A \\) and \\( B \\) occurring, and \\( P(B) \\) is the probability of event \\( B \\).\n",
    "\n",
    "### Bayes' Theorem\n",
    "- **Definition**: Bayes' theorem provides a way to update the probability of a hypothesis based on new evidence. It relates the conditional probability of an event given another event with the reverse conditional probability.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  P(H \\mid E) = \\frac{P(E \\mid H) \\cdot P(H)}{P(E)}\n",
    "  \\]\n",
    "  where:\n",
    "  - \\( P(H \\mid E) \\) is the posterior probability (the probability of hypothesis \\( H \\) given evidence \\( E \\)).\n",
    "  - \\( P(E \\mid H) \\) is the likelihood (the probability of evidence \\( E \\) given hypothesis \\( H \\)).\n",
    "  - \\( P(H) \\) is the prior probability (the initial probability of hypothesis \\( H \\)).\n",
    "  - \\( P(E) \\) is the marginal likelihood or evidence (the total probability of observing evidence \\( E \\)).\n",
    "\n",
    "### Relationship Between Bayes' Theorem and Conditional Probability\n",
    "1. **Bayes' Theorem Uses Conditional Probability**:\n",
    "   - Bayes' theorem is built on the concept of conditional probability. It provides a method to calculate the conditional probability \\( P(H \\mid E) \\) based on the reverse conditional probability \\( P(E \\mid H) \\).\n",
    "\n",
    "2. **Formulas Derivation**:\n",
    "   - Bayes' theorem can be derived from the definition of conditional probability. By using the definition:\n",
    "     \\[\n",
    "     P(H \\mid E) = \\frac{P(H \\cap E)}{P(E)}\n",
    "     \\]\n",
    "     and knowing that:\n",
    "     \\[\n",
    "     P(H \\cap E) = P(E \\mid H) \\cdot P(H)\n",
    "     \\]\n",
    "     we get:\n",
    "     \\[\n",
    "     P(H \\mid E) = \\frac{P(E \\mid H) \\cdot P(H)}{P(E)}\n",
    "     \\]\n",
    "\n",
    "3. **Updating Probabilities**:\n",
    "   - Bayes' theorem is used to update the probability of a hypothesis based on new evidence, reflecting how conditional probabilities can change as new information becomes available.\n",
    "\n",
    "4. **Reversibility**:\n",
    "   - Bayes' theorem shows how the conditional probability \\( P(H \\mid E) \\) can be calculated from the conditional probability \\( P(E \\mid H) \\), highlighting the reversible nature of conditional relationships.\n",
    "\n",
    "### Example\n",
    "Suppose you want to find the probability of having a disease (hypothesis) given a positive test result (evidence). Conditional probability tells you the probability of a positive test result given the presence of the disease (\\( P(\\text{Positive Test} \\mid \\text{Disease}) \\)). Bayes' theorem uses this information along with the prior probability of having the disease and the overall probability of a positive test result to update and calculate the probability of having the disease given a positive test result.\n",
    "\n",
    "In summary, Bayes' theorem applies the concept of conditional probability to update our beliefs about a hypothesis based on new evidence, demonstrating the practical use of conditional probabilities in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f05d0-58f1-4610-b997-7518ad774b18",
   "metadata": {},
   "source": [
    "## Question 5: How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715767d4-5319-493b-9897-dd4e7a8bb486",
   "metadata": {},
   "source": [
    "Choosing the appropriate type of Naive Bayes classifier for a given problem depends on the nature of the features in your dataset. The primary types of Naive Bayes classifiers are:\n",
    "\n",
    "1. **Gaussian Naive Bayes**\n",
    "2. **Multinomial Naive Bayes**\n",
    "3. **Bernoulli Naive Bayes**\n",
    "\n",
    "### 1. Gaussian Naive Bayes\n",
    "- **Suitable for**: Continuous features that are normally distributed.\n",
    "- **Assumption**: Assumes that the features follow a Gaussian (normal) distribution.\n",
    "- **Use case**: When your features are continuous and you expect them to have a bell-shaped distribution. For example, it works well with datasets where features like height, weight, or temperature are measured.\n",
    "- **Example**: Predicting whether a student will pass or fail based on continuous features such as exam scores or study hours.\n",
    "\n",
    "### 2. Multinomial Naive Bayes\n",
    "- **Suitable for**: Discrete features that represent counts or frequencies.\n",
    "- **Assumption**: Assumes that features are discrete and follow a multinomial distribution. This type is commonly used for text classification where the features are word counts or term frequencies.\n",
    "- **Use case**: When your features are counts or frequencies of categorical data. For instance, in text classification, where features are word counts in documents.\n",
    "- **Example**: Classifying emails as spam or not spam based on word frequencies.\n",
    "\n",
    "### 3. Bernoulli Naive Bayes\n",
    "- **Suitable for**: Binary features (features that take on only two values, such as 0 or 1).\n",
    "- **Assumption**: Assumes that features are binary and follow a Bernoulli distribution.\n",
    "- **Use case**: When your features are binary indicators, such as whether a specific word appears in a document (presence or absence) rather than its frequency.\n",
    "- **Example**: Classifying whether a document is about sports or politics based on the presence or absence of specific words.\n",
    "\n",
    "### Factors to Consider When Choosing a Naive Bayes Classifier\n",
    "\n",
    "1. **Nature of Features**:\n",
    "   - **Continuous features**: Use Gaussian Naive Bayes.\n",
    "   - **Count-based features**: Use Multinomial Naive Bayes.\n",
    "   - **Binary features**: Use Bernoulli Naive Bayes.\n",
    "\n",
    "2. **Data Distribution**:\n",
    "   - If your continuous data roughly follows a normal distribution, Gaussian Naive Bayes is a good choice.\n",
    "   - If your data involves counts or frequencies, Multinomial Naive Bayes is typically more appropriate.\n",
    "   - If your features are binary, Bernoulli Naive Bayes should be used.\n",
    "\n",
    "3. **Problem Domain**:\n",
    "   - In text classification, Multinomial Naive Bayes is often preferred due to its suitability for count-based features (e.g., term frequency).\n",
    "   - For predicting outcomes with numerical features that are normally distributed, Gaussian Naive Bayes is more appropriate.\n",
    "   - For tasks involving binary features or attributes (e.g., presence/absence of specific conditions), Bernoulli Naive Bayes is used.\n",
    "\n",
    "4. **Data Characteristics**:\n",
    "   - Analyze your data to understand its distribution and the nature of features.\n",
    "   - Consider preprocessing steps, such as discretizing continuous features or transforming binary features, to fit the assumptions of the chosen Naive Bayes classifier.\n",
    "\n",
    "### Summary\n",
    "- **Gaussian Naive Bayes**: Use for continuous, normally distributed features.\n",
    "- **Multinomial Naive Bayes**: Use for discrete count-based features.\n",
    "- **Bernoulli Naive Bayes**: Use for binary features.\n",
    "\n",
    "Selecting the appropriate Naive Bayes classifier involves understanding the distribution and type of your features and aligning them with the assumptions of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca60a5-07f8-4423-8959-beb1fd7e5729",
   "metadata": {},
   "source": [
    "## Question 6: Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "\n",
    "A 3 3 4 4 3 3 3\n",
    "\n",
    "B 2 2 1 2 2 2 3\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b64d67-9c69-4492-bf7d-a1d1161edee6",
   "metadata": {},
   "source": [
    "To classify the new instance using Naive Bayes, we'll follow these steps:\n",
    "\n",
    "1. **Calculate the likelihood of the features given each class.**\n",
    "2. **Apply Bayes' theorem to compute the posterior probability for each class.**\n",
    "3. **Compare the posterior probabilities and choose the class with the higher probability.**\n",
    "\n",
    "### Given Data\n",
    "\n",
    "- **Features and their frequencies:**\n",
    "\n",
    "  | Class | X1=1 | X1=2 | X1=3 | X2=1 | X2=2 | X2=3 | X2=4 |\n",
    "  |-------|------|------|------|------|------|------|------|\n",
    "  | A     | 3    | 3    | 4    | 4    | 3    | 3    | 3    |\n",
    "  | B     | 2    | 2    | 1    | 2    | 2    | 2    | 3    |\n",
    "\n",
    "- **New instance**: \\( X1 = 3 \\) and \\( X2 = 4 \\)\n",
    "- **Assuming equal prior probabilities** for each class: \\( P(A) = P(B) \\)\n",
    "\n",
    "### Step 1: Calculate the Likelihood\n",
    "\n",
    "**For Class A:**\n",
    "- \\( P(X1 = 3 \\mid A) \\) = Frequency of \\( X1 = 3 \\) for Class A / Total number of \\( X1 \\) values for Class A\n",
    "  \\[\n",
    "  P(X1 = 3 \\mid A) = \\frac{4}{3+3+4} = \\frac{4}{10} = 0.4\n",
    "  \\]\n",
    "- \\( P(X2 = 4 \\mid A) \\) = Frequency of \\( X2 = 4 \\) for Class A / Total number of \\( X2 \\) values for Class A\n",
    "  \\[\n",
    "  P(X2 = 4 \\mid A) = \\frac{3}{4+3+3+3} = \\frac{3}{13} \\approx 0.231\n",
    "  \\]\n",
    "\n",
    "**For Class B:**\n",
    "- \\( P(X1 = 3 \\mid B) \\) = Frequency of \\( X1 = 3 \\) for Class B / Total number of \\( X1 \\) values for Class B\n",
    "  \\[\n",
    "  P(X1 = 3 \\mid B) = \\frac{1}{2+2+1} = \\frac{1}{5} = 0.2\n",
    "  \\]\n",
    "- \\( P(X2 = 4 \\mid B) \\) = Frequency of \\( X2 = 4 \\) for Class B / Total number of \\( X2 \\) values for Class B\n",
    "  \\[\n",
    "  P(X2 = 4 \\mid B) = \\frac{3}{2+2+2+3} = \\frac{3}{9} = 0.333\n",
    "  \\]\n",
    "\n",
    "### Step 2: Apply Bayes' Theorem\n",
    "\n",
    "Since we assume equal prior probabilities \\( P(A) = P(B) \\), the posterior probability for each class is proportional to the product of the likelihoods.\n",
    "\n",
    "**For Class A:**\n",
    "\\[\n",
    "P(A \\mid X1 = 3, X2 = 4) \\propto P(X1 = 3 \\mid A) \\times P(X2 = 4 \\mid A) = 0.4 \\times 0.231 \\approx 0.092\n",
    "\\]\n",
    "\n",
    "**For Class B:**\n",
    "\\[\n",
    "P(B \\mid X1 = 3, X2 = 4) \\propto P(X1 = 3 \\mid B) \\times P(X2 = 4 \\mid B) = 0.2 \\times 0.333 \\approx 0.067\n",
    "\\]\n",
    "\n",
    "### Step 3: Compare Posterior Probabilities\n",
    "\n",
    "- **Class A**: 0.092\n",
    "- **Class B**: 0.067\n",
    "\n",
    "Since \\( P(A \\mid X1 = 3, X2 = 4) > P(B \\mid X1 = 3, X2 = 4) \\), the Naive Bayes classifier would predict that the new instance belongs to **Class A**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d97eb-d0c8-4b82-902b-7e7f62c3eef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
