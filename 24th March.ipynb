{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e7d52e-d674-4c6e-8030-97a4176f127d",
   "metadata": {},
   "source": [
    "## Question 1: What are the key features of the wine quality data set? Discuss the importance of each feature in predicting the quality of wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ad04f-8b63-458a-b744-ab76f413ff76",
   "metadata": {},
   "source": [
    "The wine quality dataset typically includes several features that describe the chemical properties of wine. Here are the key features commonly found in such datasets, along with their importance in predicting wine quality:\n",
    "\n",
    "### Key Features:\n",
    "1. Fixed Acidity:\n",
    "\n",
    "* Definition: Measures the concentration of non-volatile acids in wine, such as tartaric, malic, and citric acids.\n",
    "* Importance: Influences the taste and stability of the wine. Higher acidity can contribute to a crisp and fresh flavor, while too much acidity can result in an unpleasant sour taste.\n",
    "\n",
    "2. Volatile Acidity:\n",
    "\n",
    "* Definition: Measures the concentration of acetic acid and other volatile acids.\n",
    "* Importance: High levels of volatile acidity can indicate spoilage and may give the wine an undesirable vinegar-like taste. It is crucial for assessing the quality and freshness of the wine.\n",
    "\n",
    "3. Citric Acid:\n",
    "\n",
    "* Definition: A type of organic acid found in citrus fruits.\n",
    "* Importance: Contributes to the freshness and flavor of the wine. It can also influence the stability of the wine and its ability to age well.\n",
    "\n",
    "4. Residual Sugar:\n",
    "\n",
    "* Definition: The amount of sugar left in the wine after fermentation.\n",
    "* Importance: Affects the sweetness of the wine. The level of residual sugar is crucial for balancing the acidity and overall flavor profile of the wine.\n",
    "\n",
    "5. Chlorides:\n",
    "\n",
    "* Definition: Measures the concentration of chloride ions in the wine.\n",
    "* Importance: Influences the taste and can affect the balance of the wine. High levels of chlorides can contribute to a salty or briny taste.\n",
    "\n",
    "6. Free Sulfur Dioxide:\n",
    "\n",
    "* Definition: The amount of sulfur dioxide that is free and available to act as an antimicrobial agent.\n",
    "* Importance: Acts as a preservative and helps prevent spoilage. The right balance is important for maintaining the wine’s quality and extending its shelf life.\n",
    "\n",
    "7. Total Sulfur Dioxide:\n",
    "\n",
    "* Definition: The total amount of sulfur dioxide present in the wine, including both free and bound forms.\n",
    "* Importance: Similar to free sulfur dioxide, it helps in preserving the wine but in higher concentrations, it can impact the taste and aroma.\n",
    "\n",
    "8. Density:\n",
    "\n",
    "* Definition: The mass per unit volume of the wine.\n",
    "* Importance: Reflects the concentration of various components in the wine. It can provide insights into the wine’s body and mouthfeel.\n",
    "\n",
    "9. pH:\n",
    "\n",
    "* Definition: Measures the acidity or alkalinity of the wine.\n",
    "* Importance: Affects the taste, stability, and aging potential of the wine. pH is crucial for the balance of the wine and influences its overall quality.\n",
    "\n",
    "10. Sulphates:\n",
    "\n",
    "* Definition: The concentration of sulfate ions in the wine.\n",
    "* Importance: Contributes to the wine’s flavor and stability. Higher levels can enhance the taste and aroma but excessive amounts might cause unpleasant flavors.\n",
    "\n",
    "11. Alcohol:\n",
    "\n",
    "* Definition: The concentration of ethanol in the wine.\n",
    "* Importance: Affects the body, mouthfeel, and flavor profile of the wine. It is an important factor in defining the style and quality of the wine.\n",
    "\n",
    "12. Quality:\n",
    "\n",
    "* Definition: The quality rating of the wine, usually on a scale (e.g., 0 to 10).\n",
    "* Importance: The target variable that the model aims to predict. It is based on the overall perception of the wine, taking into account all other features.\n",
    "\n",
    "### Importance in Predicting Wine Quality:\n",
    "* Predictive Power: Features like alcohol content, pH, and volatile acidity are often highly correlated with the quality of the wine. These features help capture the overall balance and taste profile of the wine.\n",
    "* Balance and Stability: Features related to acidity, sulfur dioxide, and sugar levels are crucial for understanding how well the wine is balanced and its stability over time.\n",
    "* Flavor Profile: Elements like residual sugar, citric acid, and chlorides influence the flavor and aroma, which are significant determinants of wine quality.\n",
    "* Preservation: Sulfur dioxide levels are important for preservation and preventing spoilage, which directly affects the quality of the wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762aef53-389f-4a2b-885e-2be23ecd57f9",
   "metadata": {},
   "source": [
    "## Question 2: How did you handle missing data in the wine quality data set during the feature engineering process? Discuss the advantages and disadvantages of different imputation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400f543-b50c-49ec-85f2-eb3f213853be",
   "metadata": {},
   "source": [
    "Handling missing data is a critical step in feature engineering, as it can significantly impact the quality of your predictive models. Here’s how you might approach missing data in the wine quality dataset, along with the advantages and disadvantages of various imputation techniques:\n",
    "\n",
    "### Handling Missing Data\n",
    "1. Identifying Missing Data:\n",
    "\n",
    "* Step: First, check for missing values in the dataset.\n",
    "* Method: Use methods like df.isnull().sum() to identify columns with missing data.\n",
    "\n",
    "2. Imputation Techniques:\n",
    "\n",
    "### a. Mean/Median Imputation:\n",
    "\n",
    "* Method: Replace missing values with the mean or median of the respective column.\n",
    "\n",
    "* Advantages:\n",
    "\n",
    "* Simple and easy to implement.\n",
    "* Suitable for numerical features with missing values that are missing completely at random.\n",
    "\n",
    "* Disadvantages:\n",
    "\n",
    "* Can introduce bias if the missing data is not missing at random.\n",
    "* Reduces variability and may distort relationships between features.\n",
    "\n",
    "### b. Mode Imputation:\n",
    "\n",
    "* Method: Replace missing values with the most frequent value (mode) of the column.\n",
    "\n",
    "* Advantages:\n",
    "\n",
    "* Useful for categorical data.\n",
    "* Maintains the distribution of the data for categorical features.\n",
    "\n",
    "* Disadvantages:\n",
    "\n",
    "* May not be appropriate if there are multiple modes or if the data is not missing at random.\n",
    "\n",
    "### c. Imputation with a Constant Value:\n",
    "\n",
    "* Method: Replace missing values with a constant value, such as zero or a specific placeholder.\n",
    "\n",
    "* Advantages:\n",
    "\n",
    "* Simple and straightforward.\n",
    "\n",
    "* Disadvantages:\n",
    "\n",
    "* May not reflect the true underlying distribution of the data.\n",
    "* Can introduce a new bias if the constant value is not meaningful.\n",
    "\n",
    "### d. Predictive Imputation:\n",
    "\n",
    "* Method: Use regression models or machine learning algorithms to predict and fill missing values based on other features.\n",
    "\n",
    "* Advantages:\n",
    "\n",
    "* Can provide more accurate imputations based on relationships between features.\n",
    "* Useful for complex datasets where simple imputation methods are not sufficient.\n",
    "\n",
    "* Disadvantages:\n",
    "\n",
    "* More complex and computationally intensive.\n",
    "* Requires a well-trained model and can introduce additional uncertainty if the model is not accurate.\n",
    "\n",
    "### e. K-Nearest Neighbors (KNN) Imputation:\n",
    "\n",
    "* Method: Replace missing values based on the values of the k-nearest neighbors.\n",
    "\n",
    "* Advantages:\n",
    "\n",
    "* Takes into account the similarity between instances.\n",
    "* Can provide a more contextually appropriate imputation.\n",
    "\n",
    "* Disadvantages:\n",
    "\n",
    "* Computationally expensive, especially for large datasets.\n",
    "* Performance depends on the choice of k and the distance metric used.\n",
    "\n",
    "### f. Interpolation:\n",
    "\n",
    "* Method: Use interpolation techniques (e.g., linear interpolation) to estimate missing values based on the values of adjacent data points.\n",
    "\n",
    "* Advantages:\n",
    "\n",
    "* Useful for time-series or ordered data where values are expected to change gradually.\n",
    "\n",
    "* Disadvantages:\n",
    "\n",
    "* May not be appropriate for categorical data or data with abrupt changes.\n",
    "\n",
    "### g. Deletion:\n",
    "\n",
    "* Method: Remove rows or columns with missing values.\n",
    "\n",
    "* Advantages:\n",
    "\n",
    "* Simple and avoids the potential biases introduced by imputation.\n",
    "\n",
    "* Disadvantages:\n",
    "\n",
    "* Can result in loss of valuable data, especially if missing values are widespread.\n",
    "* May reduce the size of the dataset, impacting the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24fccd8-e39a-4e65-b0cf-4d7054c4ce27",
   "metadata": {},
   "source": [
    "## Question 3: What are the key factors that affect students' performance in exams? How would you go about analyzing these factors using statistical techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85afef28-2cec-408e-95bf-cdeca2b99b63",
   "metadata": {},
   "source": [
    "Analyzing factors that affect students' performance in exams involves examining a range of potential influences and using statistical techniques to identify significant predictors. Here’s a structured approach to analyzing these factors:\n",
    "\n",
    "### Key Factors Affecting Students' Performance\n",
    "1. Study Habits:\n",
    "\n",
    "* Factors: Hours of study per week, study environment, study methods.\n",
    "* Analysis: Correlate study habits with exam scores to see if there is a significant relationship.\n",
    "\n",
    "2. Attendance:\n",
    "\n",
    "* Factors: Number of classes attended, attendance percentage.\n",
    "* Analysis: Analyze the impact of attendance on performance using regression analysis.\n",
    "\n",
    "3. Previous Academic Performance:\n",
    "\n",
    "* Factors: Grades in previous subjects or courses.\n",
    "* Analysis: Use historical performance data to predict current exam results.\n",
    "\n",
    "4. Parental Involvement:\n",
    "\n",
    "* Factors: Level of support, involvement in school activities.\n",
    "* Analysis: Evaluate the correlation between parental involvement and student performance.\n",
    "\n",
    "5. Socioeconomic Status:\n",
    "\n",
    "* Factors: Family income, access to educational resources.\n",
    "* Analysis: Use statistical tests to assess the impact of socioeconomic factors on performance.\n",
    "\n",
    "6. Health and Well-being:\n",
    "\n",
    "* Factors: Mental health, physical health, sleep patterns.\n",
    "* Analysis: Examine how health-related factors correlate with exam scores.\n",
    "\n",
    "7. Motivation and Attitude:\n",
    "\n",
    "* Factors: Student motivation, attitude towards the subject.\n",
    "* Analysis: Use surveys or questionnaires to measure motivation and analyze its impact on performance.\n",
    "\n",
    "8. Teacher Quality:\n",
    "\n",
    "* Factors: Teacher qualifications, teaching methods.\n",
    "* Analysis: Assess how variations in teacher quality affect student outcomes.\n",
    "* Analyzing Factors Using Statistical Techniques\n",
    "\n",
    "9. Descriptive Statistics:\n",
    "\n",
    "* Purpose: Summarize and describe the features of the dataset.\n",
    "* Techniques: Calculate means, medians, standard deviations, and distributions for key variables.\n",
    "\n",
    "10. Correlation Analysis:\n",
    "\n",
    "* Purpose: Identify relationships between variables.\n",
    "* Techniques: Compute Pearson or Spearman correlation coefficients to determine the strength and direction of relationships.\n",
    "\n",
    "11. Regression Analysis:\n",
    "\n",
    "* Purpose: Understand how multiple factors influence exam performance.\n",
    "* Techniques: Perform linear regression to model the relationship between independent variables (e.g., study habits, attendance) and the dependent variable (exam scores). Multiple regression can be used to analyze the impact of several factors simultaneously.\n",
    "\n",
    "12. ANOVA (Analysis of Variance):\n",
    "\n",
    "* Purpose: Compare means across different groups.\n",
    "* Techniques: Use ANOVA to analyze if there are significant differences in exam scores among different groups based on categorical factors (e.g., different levels of parental involvement).\n",
    "\n",
    "13. Chi-Square Test:\n",
    "\n",
    "* Purpose: Examine the association between categorical variables.\n",
    "* Techniques: Use the Chi-Square test to assess the relationship between categorical variables (e.g., level of motivation and performance categories).\n",
    "\n",
    "14. Principal Component Analysis (PCA):\n",
    "\n",
    "* Purpose: Reduce dimensionality and identify key factors.\n",
    "* Techniques: Apply PCA to identify the most significant factors affecting performance by reducing the number of variables while retaining most of the variance.\n",
    "\n",
    "15. Cluster Analysis:\n",
    "\n",
    "* Purpose: Group students based on similar characteristics.\n",
    "* Techniques: Use clustering methods to identify groups of students with similar performance patterns and explore common factors within each group.\n",
    "\n",
    "16. Survival Analysis:\n",
    "\n",
    "* Purpose: Analyze time-to-event data (e.g., time until a student achieves a particular score).\n",
    "* Techniques: Use survival analysis to understand the impact of various factors on the time it takes for students to achieve specific performance goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3861bcdd-0ec2-4e61-96d6-a0c788c9749d",
   "metadata": {},
   "source": [
    "## Question 4: Describe the process of feature engineering in the context of the student performance data set. How did you select and transform the variables for your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9eee9-ae14-4deb-a4ef-93e256c5f4f0",
   "metadata": {},
   "source": [
    "Feature engineering involves the process of selecting, modifying, and creating features (variables) from raw data to improve the performance of machine learning models. In the context of a student performance dataset, this process helps in identifying the most relevant features and transforming them into formats suitable for modeling. Here’s how you can approach feature engineering for a student performance dataset:\n",
    "\n",
    "### Process of Feature Engineering\n",
    "1. Understanding the Dataset:\n",
    "\n",
    "* Objective: Gain a thorough understanding of the dataset, including the types of features available and their potential relevance to predicting student performance.\n",
    "* Actions: Examine the dataset to identify features such as student demographics, study habits, attendance records, previous grades, and other factors.\n",
    "\n",
    "2. Data Cleaning:\n",
    "\n",
    "* Objective: Prepare the data for analysis by addressing any issues such as missing values, outliers, and inconsistencies.\n",
    "* Actions:\n",
    "\n",
    "* Handle missing values through imputation or by removing incomplete records.\n",
    "* Detect and address outliers that may distort the analysis.\n",
    "* Normalize or standardize features if necessary to ensure consistency.\n",
    "\n",
    "3. Feature Selection:\n",
    "\n",
    "* Objective: Identify the most relevant features that significantly contribute to the model's predictive power.\n",
    "* Actions:\n",
    "\n",
    "* Use statistical techniques (e.g., correlation analysis) to assess the relationship between features and the target variable (e.g., exam scores).\n",
    "* Apply feature selection methods such as Filter, Wrapper, or Embedded methods to choose the most impactful features.\n",
    "\n",
    "4. Feature Transformation:\n",
    "\n",
    "* Objective: Convert raw features into more meaningful formats that can enhance model performance.\n",
    "* Actions:\n",
    "\n",
    "* Scaling: Normalize or standardize continuous features to ensure they are on a similar scale.\n",
    "* Encoding: Convert categorical variables into numerical formats using techniques like one-hot encoding or label encoding.\n",
    "* Creating Interaction Features: Combine features to capture interactions (e.g., creating a new feature representing the interaction between study hours and attendance).\n",
    "* Binning: Convert continuous variables into categorical bins if it improves model performance (e.g., binning exam scores into performance categories).\n",
    "\n",
    "5. Feature Creation:\n",
    "\n",
    "* Objective: Generate new features that may provide additional insights or predictive power.\n",
    "* Actions:\n",
    "\n",
    "* Aggregating Features: Create summary statistics like average study hours per week or total number of absences.\n",
    "* Derived Features: Generate features based on domain knowledge, such as creating a feature representing whether a student has a tutor.\n",
    "\n",
    "6. Feature Evaluation:\n",
    "\n",
    "* Objective: Assess the impact of newly engineered features on model performance.\n",
    "* Actions:\n",
    "\n",
    "* Train and evaluate models using different sets of features to determine which ones contribute most to predictive accuracy.\n",
    "* Use techniques like cross-validation to ensure that feature engineering improves model generalization.\n",
    "\n",
    "### Example of Feature Engineering\n",
    "\n",
    "Consider a student performance dataset with the following features: study_hours, attendance_rate, previous_grades, parental_support, and sleep_hours.\n",
    "\n",
    "1. Data Cleaning:\n",
    "\n",
    "* Handle missing values in parental_support by imputing with the most common value.\n",
    "* Remove outliers in study_hours if they are unusually high or low.\n",
    "\n",
    "2. Feature Selection:\n",
    "\n",
    "* Calculate the correlation between each feature and exam scores. Select features with high correlation.\n",
    "\n",
    "3. Feature Transformation:\n",
    "\n",
    "* Normalize study_hours and attendance_rate.\n",
    "* Convert parental_support (a categorical feature) into a numerical format using one-hot encoding.\n",
    "\n",
    "4. Feature Creation:\n",
    "\n",
    "* Create an interaction feature study_hours * attendance_rate to capture the combined effect of study habits and attendance.\n",
    "* Generate a new feature average_sleep if sleep_hours varies over a week.\n",
    "\n",
    "5. Feature Evaluation:\n",
    "\n",
    "* Train models using the original features and the newly engineered features. Compare performance metrics (e.g., accuracy, RMSE) to determine the effectiveness of the engineered features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8f7ee-e6f8-4895-85d7-37c76dc010e2",
   "metadata": {},
   "source": [
    "## Question 5: Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to these features to improve normality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee9e3f-2ccb-4b78-8214-6089c388c1b4",
   "metadata": {},
   "source": [
    "To perform Exploratory Data Analysis (EDA) on the wine quality dataset and identify the distribution of each feature, follow these steps:\n",
    "\n",
    "1. Load the Data\n",
    "\n",
    "Assuming the wine quality dataset is available as a CSV file, load it into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc40b46-50e4-4059-930e-40098dbc07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('wine_quality.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f95010-ff6e-40ff-9504-095526d45990",
   "metadata": {},
   "source": [
    "2. Initial Data Inspection\n",
    "\n",
    "Examine the first few rows and basic information about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c6f3c-238d-4b7a-8db3-b64342b0a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Display summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Display data types and null values\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039fc704-e0c2-43cc-99f2-fe0bd7cefc3d",
   "metadata": {},
   "source": [
    "3. Distribution of Each Feature\n",
    "\n",
    "Use visualization libraries like Matplotlib and Seaborn to plot the distribution of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9db571-5860-4f53-b6e7-f90fc2a28616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot histograms for each feature\n",
    "df.hist(figsize=(12, 10), bins=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Alternatively, use seaborn for more customized plots\n",
    "for column in df.columns:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.histplot(df[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f48155-cb54-48a4-841f-ff53d17a59ed",
   "metadata": {},
   "source": [
    "4. Assess Normality\n",
    "\n",
    "To assess normality, you can use visual methods (e.g., histograms, Q-Q plots) and statistical tests (e.g., Shapiro-Wilk test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24a7d0-5787-4d96-976c-2022712355d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Q-Q plots for each feature\n",
    "for column in df.columns:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    stats.probplot(df[column], dist=\"norm\", plot=plt)\n",
    "    plt.title(f'Q-Q Plot of {column}')\n",
    "    plt.show()\n",
    "\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "for column in df.columns:\n",
    "    stat, p = stats.shapiro(df[column].dropna())\n",
    "    print(f'{column}: Statistics={stat}, p-value={p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee24b6c-e2db-4694-9f79-2ca030ebf308",
   "metadata": {},
   "source": [
    "5. Identify Non-Normal Features\n",
    "\n",
    "Features exhibiting non-normality will show a p-value below a threshold (e.g., 0.05) in the Shapiro-Wilk test. These features might also display skewed distributions in the histograms and Q-Q plots.\n",
    "\n",
    "6. Apply Transformations\n",
    "\n",
    "To improve normality, consider applying the following transformations to non-normal features:\n",
    "\n",
    "* Log Transformation: Useful for right-skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644eb7b-814e-4f76-976f-f7a7fd7f871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_feature'] = df['non_normal_feature'].apply(lambda x: np.log(x + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4114f9-933c-4f83-a7aa-eca6fcff8db9",
   "metadata": {},
   "source": [
    "* Square Root Transformation: Useful for right-skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17343ce-e8ad-4113-9597-2647a2e37e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sqrt_feature'] = df['non_normal_feature'].apply(lambda x: np.sqrt(x + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ccbfc-d2d4-43db-8e84-09d97dfd11dc",
   "metadata": {},
   "source": [
    "* Box-Cox Transformation: Useful for stabilizing variance and making the data more normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbe1ec-6f0d-417a-bbb2-e3fcd236b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "df['boxcox_feature'], _ = stats.boxcox(df['non_normal_feature'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fcc28-a86b-41ca-bb44-74ba9a870e9d",
   "metadata": {},
   "source": [
    "* Yeo-Johnson Transformation: A variant of the Box-Cox transformation that handles zero and negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a7004-82fb-407b-b9ab-01738ec82e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "df['yeojohnson_feature'] = transformer.fit_transform(df[['non_normal_feature']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75161b-e4da-4232-911d-683163a0d14e",
   "metadata": {},
   "source": [
    "## Question 6: Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of features. What is the minimum number of principal components required to explain 90% of the variance in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d91be-6466-4c34-9f6d-c24d4b412493",
   "metadata": {},
   "source": [
    "To perform Principal Component Analysis (PCA) on the wine quality dataset and determine the minimum number of principal components required to explain 90% of the variance, follow these steps:\n",
    "\n",
    "1. Load the Data\n",
    "\n",
    "Load the wine quality dataset into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90039f-d39e-4757-aaeb-4d6433ce29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('wine_quality.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('quality', axis=1)  # Assuming 'quality' is the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe351f6-e965-4420-b14f-8273bcc08127",
   "metadata": {},
   "source": [
    "2. Standardize the Data\n",
    "\n",
    "Standardize the features to have mean 0 and variance 1, which is important for PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150e8c5-07c5-4980-bcbc-3b664ed50dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286c30f-c5fe-450c-9ba9-25bb61de78d1",
   "metadata": {},
   "source": [
    "3. Perform PCA\n",
    "\n",
    "Use PCA to reduce the dimensionality and calculate the explained variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766eb85-bac2-4f61-96fd-194ea7e761da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cumulative_variance, marker='o')\n",
    "plt.title('Cumulative Explained Variance vs. Number of Principal Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.90, color='r', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a78d56-85cf-43aa-b492-c049b771f91a",
   "metadata": {},
   "source": [
    "4. Determine the Minimum Number of Principal Components\n",
    "\n",
    "Find the minimum number of components required to explain at least 90% of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4fbabc-0f65-4e4d-bc76-25b054919b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of components explaining at least 90% variance\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "print(f'Minimum number of principal components to explain 90% of the variance: {n_components_90}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8313f-1674-476c-9e9c-6e8423934d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
